{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\ude80 Xoron Multimodal Model - Kaggle Setup Guide\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Install dependencies and setup kt-kernel\n",
    "2. Download the Xoron model from HuggingFace\n",
    "3. Load and run the model\n",
    "4. Test text generation\n",
    "5. Test image generation (snowy mountain)\n",
    "6. Test video generation (windy mountain)\n",
    "\n",
    "**Model:** `Backup-bdg/Xoron-Dev-MultiMoe`\n",
    "\n",
    "**Features:**\n",
    "- Full multimodal support (text, image, video, audio)\n",
    "- MoE LLM with MLA (Multi-Head Latent Attention)\n",
    "- 128K context with Ring Attention\n",
    "- TiTok 1D tokenization for vision/video\n",
    "- Conformer-based audio processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udce6 Step 1: Install Dependencies\n",
    "\n",
    "This cell installs all required packages including PyTorch, Transformers, and kt-kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install core dependencies\n",
    "!pip install -q transformers safetensors huggingface_hub accelerate\n",
    "\n",
    "# Install multimodal dependencies\n",
    "!pip install -q Pillow opencv-python librosa soundfile\n",
    "\n",
    "# Install CLI utilities\n",
    "!pip install -q typer rich pyyaml\n",
    "\n",
    "print(\"\u2705 Core dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udd27 Step 2: Install kt-kernel\n",
    "\n",
    "kt-kernel provides high-performance CPU/GPU inference kernels for MoE models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Install kt-kernel from the current repository\n",
    "if os.path.exists('./kt-kernel'):\n",
    "    %cd kt-kernel\n",
    "    !pip install -q -e .\n",
    "    %cd ..\n",
    "    print(\"\u2705 kt-kernel installed from local directory!\")\n",
    "else:\n",
    "    # If running from a different location, clone the repo first\n",
    "    !git clone -b feature/xoron-multimodal-support https://github.com/nigfuapp-web/xformer.git temp_xformer\n",
    "    %cd temp_xformer/kt-kernel\n",
    "    !pip install -q -e .\n",
    "    %cd ../..\n",
    "    print(\"\u2705 kt-kernel installed from GitHub!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udce5 Step 3: Download Xoron Model from HuggingFace\n",
    "\n",
    "Downloads the `Backup-bdg/Xoron-Dev-MultiMoe` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import os\n",
    "\n",
    "MODEL_REPO = \"Backup-bdg/Xoron-Dev-MultiMoe\"\n",
    "MODEL_DIR = \"./xoron-model\"\n",
    "\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    print(f\"\ud83d\udce5 Downloading model from {MODEL_REPO}...\")\n",
    "    snapshot_download(\n",
    "        repo_id=MODEL_REPO,\n",
    "        local_dir=MODEL_DIR\n",
    "    )\n",
    "    print(f\"\u2705 Model downloaded to {MODEL_DIR}\")\n",
    "else:\n",
    "    print(f\"\u2705 Model already exists at {MODEL_DIR}\")\n",
    "\n",
    "# List downloaded files\n",
    "print(\"\\n\ud83d\udcc1 Downloaded files:\")\n",
    "for f in os.listdir(MODEL_DIR)[:10]:\n",
    "    print(f\"   {f}\")\n",
    "if len(os.listdir(MODEL_DIR)) > 10:\n",
    "    print(f\"   ... and {len(os.listdir(MODEL_DIR)) - 10} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83e\udde0 Step 4: Load the Xoron Model\n",
    "\n",
    "Loads the model and processor with automatic device placement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "# Add kt-kernel to path\n",
    "kt_kernel_path = './kt-kernel/python'\n",
    "if os.path.exists(kt_kernel_path):\n",
    "    sys.path.insert(0, kt_kernel_path)\n",
    "elif os.path.exists('./temp_xformer/kt-kernel/python'):\n",
    "    sys.path.insert(0, './temp_xformer/kt-kernel/python')\n",
    "\n",
    "from kt_kernel.models.xoron import XoronForCausalLM, XoronMultimodalProcessor\n",
    "\n",
    "print(\"\ud83d\udd04 Loading Xoron model...\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Load model\n",
    "model = XoronForCausalLM.from_pretrained(\n",
    "    MODEL_DIR,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load processor\n",
    "processor = XoronMultimodalProcessor.from_pretrained(MODEL_DIR)\n",
    "\n",
    "device = next(model.parameters()).device\n",
    "print(f\"\\n\u2705 Model loaded successfully!\")\n",
    "print(f\"   Device: {device}\")\n",
    "print(f\"   Model type: {type(model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udcac Step 5: Test Text Generation\n",
    "\n",
    "Let's test the model with a simple text prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_tokens=256, temperature=0.7):\n",
    "    \"\"\"Generate text from a prompt.\"\"\"\n",
    "    inputs = processor(text=prompt, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    \n",
    "    response = processor.decode(outputs[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Test text generation\n",
    "print(\"=\"*60)\n",
    "print(\"\ud83e\udd16 TEST: Text Generation\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt = \"Hello! Tell me about yourself and your capabilities as a multimodal AI assistant.\"\n",
    "print(f\"\\n\ud83d\udcdd Prompt: {prompt}\\n\")\n",
    "\n",
    "response = generate_text(prompt)\n",
    "print(f\"\ud83d\udcac Response:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83c\udfd4\ufe0f Step 6: Generate Snowy Mountain Image\n",
    "\n",
    "Ask the model to generate an image of a mountain with lots of snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83c\udfd4\ufe0f TEST: Image Generation - Snowy Mountain\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt = \"\"\"Generate a beautiful picture of a majestic mountain peak covered with lots of pristine white snow. \n",
    "The scene should have dramatic lighting, with the sun casting golden rays on the snow-covered peaks. \n",
    "Include a clear blue sky and some pine trees at the base of the mountain. \n",
    "Make it photorealistic and highly detailed.\"\"\"\n",
    "\n",
    "print(f\"\\n\ud83d\udcdd Prompt: {prompt}\\n\")\n",
    "\n",
    "# Generate response\n",
    "response = generate_text(prompt, max_tokens=300)\n",
    "print(f\"\ud83d\udcac Response:\\n{response}\")\n",
    "\n",
    "# Try actual image generation if available\n",
    "if hasattr(model, 'generate_image') and hasattr(model.config, 'has_generator') and model.config.has_generator:\n",
    "    print(\"\\n\ud83c\udfa8 Attempting image generation...\")\n",
    "    try:\n",
    "        inputs = processor(text=prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            hidden = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "            image = model.generate_image(hidden.hidden_states[-1])\n",
    "            \n",
    "        if image is not None:\n",
    "            from PIL import Image\n",
    "            import numpy as np\n",
    "            \n",
    "            img_np = image[0].cpu().permute(1, 2, 0).numpy()\n",
    "            img_np = ((img_np + 1) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "            img = Image.fromarray(img_np)\n",
    "            img.save(\"snowy_mountain.png\")\n",
    "            print(\"\u2705 Image saved to: snowy_mountain.png\")\n",
    "            display(img)\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Image generation not available: {e}\")\n",
    "else:\n",
    "    print(\"\\n\ud83d\udcdd Note: Direct image generation requires trained generator weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83c\udf2c\ufe0f Step 7: Generate Windy Mountain Video\n",
    "\n",
    "Ask the model to generate a video of a mountain with windy climate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83c\udf2c\ufe0f TEST: Video Generation - Windy Mountain\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "prompt = \"\"\"Generate a video of a mountain landscape with windy climate. \n",
    "Show trees swaying dramatically in the strong wind, clouds moving rapidly across the sky, \n",
    "and perhaps some snow being blown off the mountain peaks. \n",
    "The atmosphere should feel dynamic and powerful, with dramatic weather conditions.\n",
    "Make it cinematic with smooth camera movement.\"\"\"\n",
    "\n",
    "print(f\"\\n\ud83d\udcdd Prompt: {prompt}\\n\")\n",
    "\n",
    "# Generate response\n",
    "response = generate_text(prompt, max_tokens=300)\n",
    "print(f\"\ud83d\udcac Response:\\n{response}\")\n",
    "\n",
    "# Try actual video generation if available\n",
    "if hasattr(model, 'generate_video') and hasattr(model.config, 'has_video_generator') and model.config.has_video_generator:\n",
    "    print(\"\\n\ud83c\udfac Attempting video generation...\")\n",
    "    try:\n",
    "        inputs = processor(text=prompt, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            hidden = model(**inputs, output_hidden_states=True, return_dict=True)\n",
    "            video = model.generate_video(hidden.hidden_states[-1], num_frames=16)\n",
    "            \n",
    "        if video is not None:\n",
    "            import cv2\n",
    "            import numpy as np\n",
    "            \n",
    "            video_np = video[0].cpu().numpy()\n",
    "            h, w = video_np.shape[2], video_np.shape[3]\n",
    "            \n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter('windy_mountain.mp4', fourcc, 8, (w, h))\n",
    "            \n",
    "            for frame in video_np:\n",
    "                frame = np.transpose(frame, (1, 2, 0))\n",
    "                frame = ((frame + 1) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "                out.write(cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))\n",
    "            \n",
    "            out.release()\n",
    "            print(\"\u2705 Video saved to: windy_mountain.mp4\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Video generation not available: {e}\")\n",
    "else:\n",
    "    print(\"\\n\ud83d\udcdd Note: Direct video generation requires trained generator weights.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83c\udfa4 Step 8: Test Audio Understanding (Optional)\n",
    "\n",
    "If you have an audio file, you can test audio understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83c\udfa4 TEST: Audio Capabilities\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if audio capabilities are available\n",
    "if hasattr(model.config, 'has_audio_encoder') and model.config.has_audio_encoder:\n",
    "    print(\"\u2705 Audio encoder is available\")\n",
    "    print(\"   - Can understand spoken language\")\n",
    "    print(\"   - Supports raw waveform input\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f Audio encoder not available in current model\")\n",
    "\n",
    "if hasattr(model.config, 'has_audio_decoder') and model.config.has_audio_decoder:\n",
    "    print(\"\u2705 Audio decoder is available\")\n",
    "    print(\"   - Can generate speech (TTS)\")\n",
    "    print(\"   - Supports zero-shot speaker cloning\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f Audio decoder not available in current model\")\n",
    "\n",
    "# Example of how to use audio input (if you have an audio file)\n",
    "print(\"\\n\ud83d\udcdd To test audio understanding, you can use:\")\n",
    "print(\"\"\"\n",
    "inputs = processor(\n",
    "    text=\"What is being said in this audio?\",\n",
    "    audio=[\"path/to/audio.wav\"],\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(processor.decode(outputs[0]))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\uddbc\ufe0f Step 9: Test Image Understanding (Optional)\n",
    "\n",
    "Test the model's ability to understand images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83d\uddbc\ufe0f TEST: Image Understanding\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check if vision capabilities are available\n",
    "if hasattr(model.config, 'has_vision_encoder') and model.config.has_vision_encoder:\n",
    "    print(\"\u2705 Vision encoder is available\")\n",
    "    print(f\"   - Vision model: {model.config.vision_model_name}\")\n",
    "    print(f\"   - TiTok enabled: {model.config.use_vision_titok}\")\n",
    "    print(f\"   - Dual-stream: {model.config.use_vision_dual_stream}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f Vision encoder not available in current model\")\n",
    "\n",
    "# Example of how to use image input\n",
    "print(\"\\n\ud83d\udcdd To analyze an image, you can use:\")\n",
    "print(\"\"\"\n",
    "from PIL import Image\n",
    "\n",
    "# Load an image\n",
    "image = Image.open(\"your_image.jpg\")\n",
    "\n",
    "# Process with the model\n",
    "inputs = processor(\n",
    "    text=\"Describe this image in detail.\",\n",
    "    images=[image],\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = {k: v.to(device) if hasattr(v, 'to') else v for k, v in inputs.items()}\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=200)\n",
    "print(processor.decode(outputs[0]))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83c\udf10 Step 10: Start API Server (Optional)\n",
    "\n",
    "You can also run the model as an API server using SGLang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83c\udf10 API Server Instructions\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "To run Xoron as an API server, open a terminal and run:\n",
    "\n",
    "```bash\n",
    "python -m sglang.launch_server \\\\\n",
    "    --model ./xoron-model \\\\\n",
    "    --host 0.0.0.0 \\\\\n",
    "    --port 8000 \\\\\n",
    "    --trust-remote-code \\\\\n",
    "    --tensor-parallel-size 1 \\\\\n",
    "    --max-total-tokens 8192 \\\\\n",
    "    --mem-fraction-static 0.85\n",
    "```\n",
    "\n",
    "Then you can make API calls:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "response = requests.post(\n",
    "    \"http://localhost:8000/v1/chat/completions\",\n",
    "    json={\n",
    "        \"model\": \"xoron\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}],\n",
    "        \"max_tokens\": 100\n",
    "    }\n",
    ")\n",
    "print(response.json())\n",
    "```\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udcca Model Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"\ud83d\udcca Model Configuration\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "config = model.config\n",
    "\n",
    "print(f\"\\n\ud83e\udde0 LLM Architecture:\")\n",
    "print(f\"   Hidden size: {config.hidden_size}\")\n",
    "print(f\"   Num layers: {config.num_layers}\")\n",
    "print(f\"   Num heads: {config.num_heads}\")\n",
    "print(f\"   Vocab size: {config.vocab_size}\")\n",
    "print(f\"   Max positions: {config.max_position_embeddings}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfaf MoE Configuration:\")\n",
    "print(f\"   Use MoE: {config.use_moe}\")\n",
    "print(f\"   Num experts: {config.num_experts}\")\n",
    "print(f\"   Experts per token: {config.num_experts_per_tok}\")\n",
    "print(f\"   MoE layer freq: {config.moe_layer_freq}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udc41\ufe0f Vision Configuration:\")\n",
    "print(f\"   Vision model: {config.vision_model_name}\")\n",
    "print(f\"   Use TiTok: {config.use_vision_titok}\")\n",
    "print(f\"   Num vision tokens: {config.num_vision_tokens}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfac Video Configuration:\")\n",
    "print(f\"   Use VideoTiTok: {config.use_video_titok}\")\n",
    "print(f\"   Max frames: {config.video_max_frames}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfa4 Audio Configuration:\")\n",
    "print(f\"   Sample rate: {config.audio_sample_rate}\")\n",
    "print(f\"   Use raw waveform: {config.use_raw_waveform}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfa8 Generation Configuration:\")\n",
    "print(f\"   Enable generation: {config.enable_generation}\")\n",
    "print(f\"   Use flow matching: {config.generation_use_flow_matching}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83e\uddf9 Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to free GPU memory\n",
    "# del model\n",
    "# del processor\n",
    "# torch.cuda.empty_cache()\n",
    "# print(\"\u2705 Memory cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## \ud83d\udcda Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. \u2705 Installing kt-kernel and dependencies\n",
    "2. \u2705 Downloading the Xoron model from HuggingFace\n",
    "3. \u2705 Loading the model with automatic device placement\n",
    "4. \u2705 Text generation capabilities\n",
    "5. \u2705 Image generation (snowy mountain)\n",
    "6. \u2705 Video generation (windy mountain)\n",
    "7. \u2705 Audio and vision understanding capabilities\n",
    "8. \u2705 API server deployment options\n",
    "\n",
    "**Repository:** https://github.com/nigfuapp-web/xformer (branch: `feature/xoron-multimodal-support`)\n",
    "\n",
    "**Model:** `Backup-bdg/Xoron-Dev-MultiMoe`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}